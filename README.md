# SAFT: Semantic-Aware Adversarial Fine-Tuning

Code repository for SAFT.

---
## Installation

### Prerequisites
- Python 3.8+
- CUDA 11.7+ or CUDA 12.x
- Conda (recommended)

### Step 1: Create Conda Environment
```bash
conda create -n SAFT python=3.8
conda activate SAFT
```

### Step 2: Install PyTorch
Install PyTorch with CUDA support (adjust CUDA version as needed):

```bash
# For CUDA 12.4
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu124

# For CUDA 11.8
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu118
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

### Step 4: Download CLIP Models
If you need to pre-download CLIP models:

```bash
bash scripts/download_models.sh
```
---

## Usage

### Training

#### Basic Training (TGA-ZSR Baseline)
Train on TinyImageNet with single template prompt:

```bash
python main.py --ncaps 1 --batch_size 128 --epochs 10 --seed 1
```

#### SAFT Training (Our Method)
Train with multiple semantic captions:

```bash
python main.py --ncaps 5 --batch_size 128 --epochs 10 --seed 1
```

#### SAFT with MLLM Descriptions
Use MLLM-generated descriptions:

```bash
python main.py --ncaps 5 --batch_size 128 --epochs 10 --seed 1 --MLLM
```

#### Training on ImageNet
```bash
python main.py --dataset ImageNet --ncaps 5 --batch_size 128 --seed 1 --epochs 1
```
---
## Experiment Tracking
The code uses **Weights & Biases (wandb)** for experiment tracking by default.

- Set `--wandb True` to enable logging (default)
- Set `--wandb False` to disable logging
- Logs include: training loss, clean/adversarial accuracy, hyperparameters

---

## Output
### Saved Models
Models are saved to `./save/models/{experiment_name}/`:

```
model_best_seed{index}.pth.tar
```
---

## Project Structure
```
SAFT/
├── main.py                  # Main entry point
├── config.py                # Configuration and argument parsing
├── training.py              # Training logic and TrainingManager
├── model_setup.py           # Model initialization and setup
├── validation.py            # Validation and adversarial evaluation
├── helper.py                # Utility functions (data loading, text prompts, etc.)
├── attacks/                 # Adversarial attack implementations
│   ├── pgd_attack.py       # PGD attack
│   ├── auto_attack.py      # AutoAttack
│   └── cw_attack.py        # C&W attack
├── models/                  # CLIP model and prompters
│   ├── clip/               # CLIP implementation
│   ├── model.py            # Model forward pass utilities
│   └── prompters.py        # Visual prompting methods
├── prompts/                 # Text prompt datasets
│   ├── cupl.json                                    # ImageNet descriptions generated by LLM
│   ├── imagenet_descriptions_by_category.json       # ImageNet descriptions generated by MLLM
│   └── tinyimagenet_descriptions_by_category.json   # TinyImageNet descriptions generated by MLLM
├── val_datasets/            # Validation dataset loaders
├── utils/                   # Utility files (class name mappings, etc.)
├── requirements.txt         # Python dependencies
└── README.md               # This file
```
---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---
